{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of English_to_Python1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGm93dHHpGrk"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?id=19sAkjvCwogvJgoDM5vlCMT1JCOW9kdUx&export=download\" -O english_python_data.txt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlv7IfhFQhsv"
      },
      "source": [
        "# Data Perparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujrKTT6IpEb0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-mOljzdcIZL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, BucketIterator, Iterator\n",
        "from torchtext.legacy import data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Yvq1bA-5OY",
        "outputId": "92fd8b45-e0d8-4fc2-8556-f6e7bc95d6f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5WZWgUq7N0Qn",
        "outputId": "0f2bc31c-ffe1-44ab-8852-faa9fdc92d6a"
      },
      "source": [
        "torchtext.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.10.0'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwv64O6zIeU",
        "outputId": "916fc235-8b0d-4e24-e14c-b9abd32fef1d"
      },
      "source": [
        "%set_env CUDA_LAUNCH_BLOCKING = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1UTpnAaSUlp"
      },
      "source": [
        "## Reading the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYRncG2sPvbS"
      },
      "source": [
        "f = open(\"english_python_data.txt\", \"r\")\n",
        "file_lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcPoWVXRYqbT",
        "outputId": "950883ea-2458-4d18-be59-8d40bef08710"
      },
      "source": [
        "file_lines[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#You are given a ASCII diagram, comprised of minus signs -, plus signs +, vertical bars | and whitespaces . Your task is to write a function which breaks the diagram in the minimal pieces it is made of. For example, if the input for your function is this diagram: +------------+ | | | | | | +------+-----+ | | | | | | +------+-----+ the returned value should be the list of: +------------+ | | | | | | +------------+ (note how it lost a + sign in the extraction) as well as +------+ | | | | +------+ and +-----+ | | | | +-----+ The diagram is given as an ordinary multiline string. There are no borders touching each others. The pieces should not have trailing spaces at the end of the lines. However, it could have leading spaces if the figure is not a rectangle. For instance: +---+ | | +---+ | | | +-------+ However, it is not allowed to use more leading spaces than necessary. It is to say, the first character of some of the lines should be different than a space. Finally, note that only the explicitly closed pieces are considered. Spaces \"outside\" of the shape are part of the background . Therefore the diagram above has a single piece. Have fun! Note : in C++ you are provided with two utility functions : cpp std::string join(const std::string &sep, const std::vector<std::string> &to_join); // Returns the concatenation of all the strings in the vector, separated with sep std::vector<std::string> split_lines(const std::string &to_split); // Splits a string, using separator \\'\\\\n\\' \\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " 'def split_lines(to_split):\\n',\n",
              " \"    return to_split.split('\\\\n')\\n\",\n",
              " '\\n',\n",
              " '\\n',\n",
              " 'def join(sep, to_join):\\n',\n",
              " '    return sep.join(to_join)\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " 'def extract(diagram):\\n',\n",
              " '    lines = split_lines(diagram)\\n',\n",
              " '\\n',\n",
              " '    pieces = []\\n',\n",
              " '\\n',\n",
              " '    for line in lines:\\n',\n",
              " '            continue\\n',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbhPC2wnSbOA"
      },
      "source": [
        "Our dataset is formulated in a manner where every question starts with '#'. Lines between two consecutive '#' forms the solution to the question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1HR4YOwXXln"
      },
      "source": [
        "dps = []\n",
        "dp = None\n",
        "for line in file_lines:\n",
        "  if line[0] == \"#\":\n",
        "    if dp:\n",
        "      dp['solution'] = ''.join(dp['solution'])\n",
        "      dps.append(dp)\n",
        "    dp = {\"question\": None, \"solution\": []}\n",
        "    dp['question'] = line[1:]\n",
        "  else:\n",
        "    dp[\"solution\"].append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvsphmddA0cU",
        "outputId": "c6fab916-57f4-46cf-fb51-43241bb6d688"
      },
      "source": [
        "print(len(dps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD7v0Bf_AuMU"
      },
      "source": [
        "dps = [dp for dp in dps if len(dp[\"solution\"]) > 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfrNiFVkBRTM",
        "outputId": "abacb68f-952a-4bfb-82cf-e9693f2189c0"
      },
      "source": [
        "len(dps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11607"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOvv0T1FaH-V",
        "outputId": "30c132f3-199d-414a-c5fd-64231df1a447"
      },
      "source": [
        "i=0\n",
        "for dp in dps:\n",
        "  print(\"\\n Question no: \", i+1)\n",
        "  i+=1\n",
        "  print(dp['question'][1:])\n",
        "  print(dp['solution'])\n",
        "  if i>49:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Question no:  1\n",
            "ou are given a ASCII diagram, comprised of minus signs -, plus signs +, vertical bars | and whitespaces . Your task is to write a function which breaks the diagram in the minimal pieces it is made of. For example, if the input for your function is this diagram: +------------+ | | | | | | +------+-----+ | | | | | | +------+-----+ the returned value should be the list of: +------------+ | | | | | | +------------+ (note how it lost a + sign in the extraction) as well as +------+ | | | | +------+ and +-----+ | | | | +-----+ The diagram is given as an ordinary multiline string. There are no borders touching each others. The pieces should not have trailing spaces at the end of the lines. However, it could have leading spaces if the figure is not a rectangle. For instance: +---+ | | +---+ | | | +-------+ However, it is not allowed to use more leading spaces than necessary. It is to say, the first character of some of the lines should be different than a space. Finally, note that only the explicitly closed pieces are considered. Spaces \"outside\" of the shape are part of the background . Therefore the diagram above has a single piece. Have fun! Note : in C++ you are provided with two utility functions : cpp std::string join(const std::string &sep, const std::vector<std::string> &to_join); // Returns the concatenation of all the strings in the vector, separated with sep std::vector<std::string> split_lines(const std::string &to_split); // Splits a string, using separator '\\n' \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def split_lines(to_split):\n",
            "    return to_split.split('\\n')\n",
            "\n",
            "\n",
            "def join(sep, to_join):\n",
            "    return sep.join(to_join)\n",
            "\n",
            "\n",
            "def extract(diagram):\n",
            "    lines = split_lines(diagram)\n",
            "\n",
            "    pieces = []\n",
            "\n",
            "    for line in lines:\n",
            "            continue\n",
            "\n",
            "\n",
            "\n",
            " Question no:  2\n",
            "write a python program to add two numbers \n",
            "\n",
            "num1 = 1.5\n",
            "num2 = 6.3\n",
            "sum = num1 + num2\n",
            "print(f'Sum: {sum}')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  3\n",
            "write a python function to add two user provided numbers and return the sum\n",
            "\n",
            "def add_two_numbers(num1, num2):\n",
            "    sum = num1 + num2\n",
            "    return sum\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  4\n",
            "write a program to find and print the largest among three numbers\n",
            "\n",
            "\n",
            "num1 = 10\n",
            "num2 = 12\n",
            "num3 = 14\n",
            "if (num1 >= num2) and (num1 >= num3):\n",
            "   largest = num1\n",
            "elif (num2 >= num1) and (num2 >= num3):\n",
            "   largest = num2\n",
            "else:\n",
            "   largest = num3\n",
            "print(f'largest:{largest}')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  5\n",
            "write a program to find and print the smallest among three numbers\n",
            "\n",
            "num1 = 10\n",
            "num2 = 12\n",
            "num3 = 14\n",
            "if (num1 <= num2) and (num1 <= num3):\n",
            "   smallest = num1\n",
            "elif (num2 <= num1) and (num2 <= num3):\n",
            "   smallest = num2\n",
            "else:\n",
            "   smallest = num3\n",
            "print(f'smallest:{smallest}')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  6\n",
            "Write a python function to merge two given lists into one\n",
            "\n",
            "def merge_lists(l1, l2):\n",
            "    return l1 + l2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  7\n",
            "Write a program to check whether a number is prime or not\n",
            "\n",
            "num = 337\n",
            "\n",
            "if num > 1:\n",
            "   for i in range(2, num//2 + 1):\n",
            "       if (num % i) == 0:\n",
            "           print(num,\"is not a prime number\")\n",
            "           print(f\"{i} times {num//i} is {num}\")\n",
            "           break\n",
            "   else:\n",
            "       print(f\"{num} is a prime number\")\n",
            "\n",
            "else:\n",
            "   print(f\"{num} is not a prime number\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  8\n",
            "Write a python function that prints the factors of a given number\n",
            "\n",
            "def print_factors(x):\n",
            "   print(f\"The factors of {x} are:\")\n",
            "   for i in range(1, x + 1):\n",
            "       if x % i == 0:\n",
            "           print(i)\n",
            "\n",
            "\n",
            "\n",
            " Question no:  9\n",
            "Write a program to find the factorial of a number\n",
            "\n",
            "num = 13\n",
            "factorial = 1\n",
            "\n",
            "if num < 0:\n",
            "   print(\"No factorials for negative numbers!\")\n",
            "\n",
            "elif num == 0:\n",
            "   print(\"The factorial of 0 is 1\")\n",
            "\n",
            "else:\n",
            "   for i in range(1,num + 1):\n",
            "       factorial = factorial*i\n",
            "   print(f\"The factorial of {num} is {factorial}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  10\n",
            "Write a python function to print whether a number is negative, positive or zero\n",
            "\n",
            "def check_pnz(num):\n",
            "    if num > 0:\n",
            "       print(\"Positive number\")\n",
            "\n",
            "    elif num == 0:\n",
            "       print(\"Zero\")\n",
            "\n",
            "    else:\n",
            "       print(\"Negative number\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  11\n",
            "Write a program to print the multiplication table of a given number\n",
            "\n",
            "\n",
            "num = 9\n",
            "for i in range(1, 11):\n",
            "   print(f\"{num} x {i} = {num*i}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  12\n",
            "Write a python function to print powers of 2, for given number of terms\n",
            "\n",
            "def two_power(terms):\n",
            "    result = list(map(lambda x: 2 ** x, range(terms)))\n",
            "\n",
            "    print(f\"The total terms are: {terms}\")\n",
            "    for i in range(terms):\n",
            "       print(f\"2^{i} = {result[i]}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  13\n",
            "Write a program to filter the numbers in a list which are divisible by a given number\n",
            "\n",
            "my_list = [11, 45, 74, 89, 132, 239, 721, 21]\n",
            "\n",
            "num = 3\n",
            "result = list(filter(lambda x: (x % num == 0), my_list))\n",
            "\n",
            "print(f\"Numbers divisible by {num} are {result}\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  14\n",
            "Write a python function that returns the sum of n natural numbers\n",
            "\n",
            "def sum_natural(num):\n",
            "    if num < 0:\n",
            "       print(\"Please enter a positive number!\")\n",
            "    else:\n",
            "       sum = 0\n",
            "       while(num > 0):\n",
            "           sum += num\n",
            "           num -= 1\n",
            "       return num\n",
            "\n",
            "\n",
            "\n",
            " Question no:  15\n",
            "Write a program to swap first and last elements in a list\n",
            "\n",
            "my_list = [1, 2, 3, 4, 5, 6]\n",
            "my_list[0], my_list[-1] = my_list[-1], my_list[0]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  16\n",
            "Write a python function to find the area of a circle, whose radius is given\n",
            "\n",
            "def findArea(r): \n",
            "    PI = 3.142\n",
            "    return PI * (r*r)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  17\n",
            "Write a program to print the sum of squares of first n natural numbers\n",
            "\n",
            "n = 21\n",
            "sum_n = 0\n",
            "for i in range(1, n+1):\n",
            "    sum_n += i**2\n",
            "print(sum_n)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  18\n",
            "Write a program to print the length of a list\n",
            "\n",
            "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "\n",
            "print(len(my_list))\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  19\n",
            "Write a pythno function to print the length of a given tuple\n",
            "\n",
            "my_tuple = (1, 2, 3, 4, 5, 6, 7, 8)\n",
            "\n",
            "print(len(my_tuple))\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  20\n",
            "Write a python function to print the elements of a given list, one element in a line\n",
            "\n",
            "def custom_print(l):\n",
            "    for _ in l:\n",
            "        print(_)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  21\n",
            "Write a python function to remove all the odd numbers from a list and return the remaining list\n",
            "\n",
            "\n",
            "def remove_odd(my_list):\n",
            "    result = list(filter(lambda x: (x % 2 == 0), my_list))\n",
            "    return result\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  22\n",
            "Write a python function to remove all the even numbers from a list and return the remaining list\n",
            "\n",
            "\n",
            "def remove_even(my_list):\n",
            "    result = list(filter(lambda x: (x % 2 != 0), my_list))\n",
            "    return result\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  23\n",
            "Write a function that takes two lists as input and returns a zipped list of corresponding elements\n",
            "\n",
            "\n",
            "def zip_list(list1, list2):\n",
            "    return list(zip(list1, list2))\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  24\n",
            "Write a program to to print the contents of a given file\n",
            "\n",
            "file_name = 'temp.txt'\n",
            "with open(file_name, 'r') as f:\n",
            "    print(f.read())\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  25\n",
            "Write a functin that returns the LCM of two input numbers\n",
            "\n",
            "\n",
            "def lcm(a, b):\n",
            "    if a>b:\n",
            "        min_ = a\n",
            "    else:\n",
            "        min_ = b\n",
            "    while True:\n",
            "        if min_%a==0 and min_%b==0:\n",
            "            break\n",
            "        min_+=1\n",
            "    return min_\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  26\n",
            "Write a program to print the unique elements in a list\n",
            "\n",
            "my_list = [1, 2, 4, 5, 2, 3, 1, 5, 4, 7, 8, 2, 4, 5, 2, 7, 3]\n",
            "\n",
            "print(set(my_list))\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  27\n",
            "Write a function that returns the sum of digits of a given number\n",
            "\n",
            "def digisum(num):\n",
            "    sum_=0\n",
            "    while num > 0:\n",
            "        dig = num % 10\n",
            "        sum_+=dig\n",
            "        num//=10\n",
            "    return sum_\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  28\n",
            "Write a program to check and print whether a number is palindrome or not\n",
            "\n",
            "\n",
            "num = 12321\n",
            "temp = num\n",
            "rev = 0\n",
            "while num > 0:\n",
            "    dig = num % 10\n",
            "    rev = rev*10 + dig\n",
            "    num//=10\n",
            "if temp==rev :\n",
            "    print(\"The number is a palindrome!\")\n",
            "else:\n",
            "    print(\"The number isn't a palindrome!\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  29\n",
            "Write a function that prints a given value, n number of times\n",
            "\n",
            "def print_n(val, n):\n",
            "    for _ in range(n):\n",
            "        print(val)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  30\n",
            "Write a function to find the area of sqaure\n",
            "\n",
            "def square_area(a):\n",
            "    return a*a\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  31\n",
            "Write a function to find the perimeter of a square\n",
            "\n",
            "def square_perimeter(a):\n",
            "    return 4*a\n",
            "\n",
            "\n",
            "\n",
            " Question no:  32\n",
            "Write a function to find the area of rectangle\n",
            "\n",
            "def rectangle_area(l, b):\n",
            "    return l*b\n",
            "\n",
            "\n",
            "\n",
            " Question no:  33\n",
            "Write a function to find the permieter of a rectangle\n",
            "\n",
            "def rectangle_perimeter(l, b):\n",
            "    return 2*(l+b)\n",
            "\n",
            "\n",
            "\n",
            " Question no:  34\n",
            "Write a python function to find the area of a circle, whose radius is given\n",
            "\n",
            "def findArea(r): \n",
            "    PI = 3.142\n",
            "    return PI * (r*r)\n",
            "\n",
            "\n",
            "\n",
            " Question no:  35\n",
            "Write a function to calculate and return electricity bill. Units used are given. Price per unit is fixed and is increased after 750 units.\n",
            "\n",
            "\n",
            "def calc_elect_bill(units):\n",
            "    if units > 0:\n",
            "        if units <= 750:\n",
            "            return 5*units\n",
            "        else:\n",
            "            return 5*(750) + 7*(units-750)\n",
            "\n",
            "    else:\n",
            "        return -1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  36\n",
            "Write a function to return day of a week, given the number\n",
            "\n",
            "def give_day(n):\n",
            "    day_dict = {1: 'Sunday', 2: 'Monday', 3: 'Tuesday', 4: 'Wednesday', 5: 'Thursday', 6: 'Friday', 7: 'Saturday'}\n",
            "    return day_dict[n]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  37\n",
            "Write a program to calculate and print the volume of a cylender\n",
            "\n",
            "r = 3\n",
            "h = 5\n",
            "pi = 3.14\n",
            "volume = pi*(r**2)*h\n",
            "print(volume)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  38\n",
            "Write a function to calculate and return the average of input numbers\n",
            "\n",
            "\n",
            "def calc_avg(*args):\n",
            "    if len(args) > 0:\n",
            "        return sum(args)/len(args)\n",
            "    return None\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  39\n",
            "Write a function to calculate compound interest, given p, r, t\n",
            "\n",
            "def comp_int(p, r, t):\n",
            "    amount = p * (1 + (r/100))**t\n",
            "    interest = amount - p\n",
            "    return interest\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  40\n",
            "Write a function to calculate simple interest, given p, r, t\n",
            "\n",
            "def simp_int(p, r, t):\n",
            "    interest = (p*r*t)/100\n",
            "    return interest\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  41\n",
            "Write a program to print a given string, replacing all the vowels with '_'\n",
            "\n",
            "\n",
            "st = \"Where is this going? Could you please help me understand!\"\n",
            "vowels = \"AEIOUaeiou\"\n",
            "\n",
            "for v in vowels:\n",
            "    st = st.replace(v, '_')\n",
            "\n",
            "print(st)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  42\n",
            "Write a functio to check whether a number if perfect or not\n",
            "\n",
            "def is_perfect(n):\n",
            "    sum_ = 0\n",
            "    for i in range(1, n//2 + 1):\n",
            "        if n%i == 0:\n",
            "            sum_+=i\n",
            "    if sum_ == n:\n",
            "        return True\n",
            "    return False\n",
            "\n",
            "\n",
            "\n",
            " Question no:  43\n",
            "Write a function that returns seperate lists of positive and negative numbers from an input list\n",
            "\n",
            "def seperate_pn(l):\n",
            "    pos_list = []\n",
            "    neg_list = []\n",
            "    for _ in l:\n",
            "        if _<0:\n",
            "            neg_list.append(_)\n",
            "        else:\n",
            "            pos_list.append(_)\n",
            "    return pos_list, neg_list\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  44\n",
            "Write a program to find and print the area of a triangle, whose hight and width are given.\n",
            "\n",
            "\n",
            "h = 12\n",
            "w = 11\n",
            "area = 0.5*h*w\n",
            "print(area)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Question no:  45\n",
            "Write a function to find acceleration, given u, v and t\n",
            "\n",
            "\n",
            "def acc(u, v, t):\n",
            "    return (v-u)/t\n",
            "\n",
            "\n",
            "\n",
            " Question no:  46\n",
            "Write a lambda function to multiply two numbers\n",
            "\n",
            "\n",
            "multiply = lambda a, b: a*b\n",
            "\n",
            "\n",
            "\n",
            " Question no:  47\n",
            "Write a lambda function to add two numbers\n",
            "\n",
            "\n",
            "add = lambda a, b: a+b\n",
            "\n",
            "\n",
            "\n",
            " Question no:  48\n",
            "Write a lambda function that gives True if the input number is even otherwise False\n",
            "\n",
            "\n",
            "even = lambda a: True if a%2 == 0 else False\n",
            "\n",
            "\n",
            "\n",
            " Question no:  49\n",
            "Write a lambda function to to give character grom it's ascii value\n",
            "\n",
            "\n",
            "ascii = lambda a: chr(a)\n",
            "\n",
            "\n",
            "\n",
            " Question no:  50\n",
            "Write a lambda function to that gives the number of digits in a number\n",
            "\n",
            "\n",
            "dig_cnt = lambda a: len(str(a))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA-1MuOecZRt",
        "outputId": "7485d029-80a1-4066-df08-7d34451a80a5"
      },
      "source": [
        "print(\"Dataset size:\", len(dps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 11607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swBfwcDaXEh5"
      },
      "source": [
        "## Using a custom tokenizer to tokenize python code\n",
        "\n",
        "Python is a programming language with its own unique syntax. Regular tokenizers like spacy are meant to tokenize english scentences and are not optimized towards Python's syntax. Here, we write our own custom tokenizer that makes use of Python's default [tokenize](https://docs.python.org/3/library/tokenize.html) library. When we make use of this library we only extract the token type and the token string.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UsevXhPgh1R"
      },
      "source": [
        "from tokenize import tokenize, untokenize\n",
        "import io\n",
        "\n",
        "\n",
        "def tokenize_python_code(python_code_str):\n",
        "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
        "    tokenized_output = []\n",
        "    for i in range(0, len(python_tokens)):\n",
        "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    return tokenized_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V-sePckv1-K",
        "outputId": "594d0766-d7b8-4885-c0ac-eacbf81b86d5"
      },
      "source": [
        "tokenized_sample = tokenize_python_code(dps[1]['solution'])\n",
        "print(tokenized_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(57, 'utf-8'), (1, 'num1'), (53, '='), (2, '1.5'), (4, '\\n'), (1, 'num2'), (53, '='), (2, '6.3'), (4, '\\n'), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'num2'), (4, '\\n'), (1, 'print'), (53, '('), (3, \"f'Sum: {sum}'\"), (53, ')'), (4, '\\n'), (56, '\\n'), (56, '\\n'), (0, '')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ00a5cwwHtL",
        "outputId": "9566ce71-607d-4239-ce55-77abea424ee1"
      },
      "source": [
        "print(untokenize(tokenized_sample).decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num1 =1.5 \n",
            "num2 =6.3 \n",
            "sum =num1 +num2 \n",
            "print (f'Sum: {sum}')\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nsoc0eCpb8e"
      },
      "source": [
        "Since we have mere 5000 data points, we make use of data augmentations to increase the size of our dataset. While tokenizing the python code, we mask the names of certain variables randomly(with 'var_1, 'var_2' etc) to ensure that the model that we train does not merly fixate on the way the variables are named and actually tries to understand the inhrent logic and syntax of the python code.\n",
        "\n",
        "But, while randomly picking varibles to mask we avoid keyword literals(*keyword.kwlist*), control structures(as can be seen in below *skip_list*) and object properties. We add all such literals that need to be skipped into the *skip_list*\n",
        "\n",
        "```skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM-tqDwhYoVJ",
        "outputId": "7a3c037d-0092-4db1-d0f9-c0f7304c22d6"
      },
      "source": [
        "import keyword\n",
        "\n",
        "print(keyword.kwlist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbIMyrYapfwS"
      },
      "source": [
        "def augment_tokenize_python_code(python_code_str, mask_factor=0.3):\n",
        "\n",
        "\n",
        "    var_dict = {} # Dictionary that stores masked variables\n",
        "\n",
        "    # certain reserved words that should not be treated as normal variables and\n",
        "    # hence need to be skipped from our variable mask augmentations\n",
        "    skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'zip'\n",
        "                 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']\n",
        "    skip_list.extend(keyword.kwlist)\n",
        "\n",
        "    var_counter = 1\n",
        "    python_tokens = list(tokenize(io.BytesIO(python_code_str.encode('utf-8')).readline))\n",
        "    tokenized_output = []\n",
        "\n",
        "    for i in range(0, len(python_tokens)):\n",
        "      if python_tokens[i].type == 1 and python_tokens[i].string not in skip_list:\n",
        "        \n",
        "        if i>0 and python_tokens[i-1].string in ['def', '.', 'import', 'raise', 'except', 'class']: # avoid masking modules, functions and error literals\n",
        "          skip_list.append(python_tokens[i].string)\n",
        "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "        elif python_tokens[i].string in var_dict:  # if variable is already masked\n",
        "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string]))\n",
        "        elif random.uniform(0, 1) > 1-mask_factor: # randomly mask variables\n",
        "          var_dict[python_tokens[i].string] = 'var_' + str(var_counter)\n",
        "          var_counter+=1\n",
        "          tokenized_output.append((python_tokens[i].type, var_dict[python_tokens[i].string]))\n",
        "        else:\n",
        "          skip_list.append(python_tokens[i].string)\n",
        "          tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "      \n",
        "      else:\n",
        "        tokenized_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    \n",
        "    return tokenized_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HJwaBJOhdxb",
        "outputId": "33c4d5d9-54a9-4466-aaa9-2ee5c0eef281"
      },
      "source": [
        "tokenized_sample = augment_tokenize_python_code(dps[1]['solution'])\n",
        "print(tokenized_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(57, 'utf-8'), (1, 'var_1'), (53, '='), (2, '1.5'), (4, '\\n'), (1, 'num2'), (53, '='), (2, '6.3'), (4, '\\n'), (1, 'sum'), (53, '='), (1, 'var_1'), (53, '+'), (1, 'num2'), (4, '\\n'), (1, 'print'), (53, '('), (3, \"f'Sum: {sum}'\"), (53, ')'), (4, '\\n'), (56, '\\n'), (56, '\\n'), (0, '')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciLm6oezs3A2",
        "outputId": "94ca167d-178c-4371-d940-78f2fbac7d3b"
      },
      "source": [
        "print(untokenize(tokenized_sample).decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "var_1 =1.5 \n",
            "num2 =6.3 \n",
            "sum =var_1 +num2 \n",
            "print (f'Sum: {sum}')\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hYArh3OznQ0"
      },
      "source": [
        "As one can see our augmented tokenizer picked num2 randomly and masked(replaced) it with by var_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shdWBNLu0DkK"
      },
      "source": [
        "## Building Train and Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhBx_KFVaJIK"
      },
      "source": [
        "python_problems_df = pd.DataFrame(dps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6i3TKNUoaWPC",
        "outputId": "ca25c950-0850-400e-cd67-02c13050c3f0"
      },
      "source": [
        "python_problems_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You are given a ASCII diagram, comprised of mi...</td>\n",
              "      <td>\\n\\n\\ndef split_lines(to_split):\\n    return t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a python function to add two user provi...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the largest...</td>\n",
              "      <td>\\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a program to find and print the smalles...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                                           solution\n",
              "0  You are given a ASCII diagram, comprised of mi...  \\n\\n\\ndef split_lines(to_split):\\n    return t...\n",
              "1       write a python program to add two numbers \\n  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "2   write a python function to add two user provi...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "3   write a program to find and print the largest...  \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >=...\n",
              "4   write a program to find and print the smalles...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n..."
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_93zNqfepxY",
        "outputId": "342527c9-5e26-472a-86da-7ce5bc93625e"
      },
      "source": [
        "python_problems_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11607, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cdHbscZsSJu"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "msk = np.random.rand(len(python_problems_df)) < 0.85 # Splitting data into 85% train and 15% validation\n",
        "\n",
        "train_df = python_problems_df[msk]\n",
        "val_df = python_problems_df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDtZSkLUu53D",
        "outputId": "97164109-701a-4036-ffbf-82bf9db6004f"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9896, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3SkbK0Qu9TZ",
        "outputId": "c2ef0595-2d89-4904-dddf-7dccdb00f0b5"
      },
      "source": [
        "val_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1711, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u7EyiRZ6brX"
      },
      "source": [
        "## Creating vocabulary using torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ZQQWsWi4rn"
      },
      "source": [
        "In this section we will use torchtext Fields to construct the vocabulary for our sequence-to-sequence learning problem.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlIi4zsLKwLE"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLID_2ZtixKY"
      },
      "source": [
        "Input = data.Field(tokenize = 'spacy',\n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True)\n",
        "\n",
        "Output = data.Field(tokenize = augment_tokenize_python_code,\n",
        "                    init_token='<sos>', \n",
        "                    eos_token='<eos>', \n",
        "                    lower=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttEpbc2rixPx"
      },
      "source": [
        "fields = [('Input', Input),('Output', Output)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxsyNTBtogcD"
      },
      "source": [
        "Since our data augmentations have the potential to increase the vocabulary beyond what it initially is, we must ensure that we capture as many variations as possible in the vocabulary that we develop. In the the below code we apply our data augmentations 100 times to ensure that we can capture a majority of augmentations into our vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP_yJeqjoLLV"
      },
      "source": [
        "train_example = []\n",
        "val_example = []\n",
        "\n",
        "train_expansion_factor = 100\n",
        "for j in range(train_expansion_factor):\n",
        "  for i in range(train_df.shape[0]):\n",
        "      try:\n",
        "          ex = data.Example.fromlist([train_df.question[i], train_df.solution[i]], fields)\n",
        "          train_example.append(ex)\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "for i in range(val_df.shape[0]):\n",
        "    try:\n",
        "        ex = data.Example.fromlist([val_df.question[i], val_df.solution[i]], fields)\n",
        "        val_example.append(ex)\n",
        "    except:\n",
        "        pass       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooT2EqpmtcAW"
      },
      "source": [
        "train_data = data.Dataset(train_example, fields)\n",
        "valid_data =  data.Dataset(val_example, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk4zu6Ntqgfr"
      },
      "source": [
        "Input.build_vocab(train_data, min_freq = 0)\n",
        "Output.build_vocab(train_data, min_freq = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA17CYnDJa4D",
        "outputId": "7df0cdc2-cc5d-46c8-f380-8a05cc7dd476"
      },
      "source": [
        "Output.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.vocab.Vocab at 0x7f8b7ccac2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5f1HbzYuEcD"
      },
      "source": [
        "def save_vocab(vocab, path):\n",
        "    import pickle\n",
        "    output = open(path, 'wb')\n",
        "    pickle.dump(vocab, output)\n",
        "    output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN6tUrOxuFac"
      },
      "source": [
        " save_vocab(Input.vocab, \"/content/drive/MyDrive/EnglishToPython/new/src_vocab.pkl\")\n",
        " save_vocab(Output.vocab, \"/content/drive/MyDrive/EnglishToPython/new/trg_vocab.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pv-5RpJuAjb",
        "outputId": "e65bfdf6-78ff-4ff2-b90e-a2ac9d1a2ac9"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgNa1xUluGHc",
        "outputId": "3cab80c4-f8cd-4863-a0ab-1c56d472a2ff"
      },
      "source": [
        "train_data[0].Output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(57, 'utf-8'),\n",
              " (56, '\\n'),\n",
              " (56, '\\n'),\n",
              " (56, '\\n'),\n",
              " (1, 'def'),\n",
              " (1, 'split_lines'),\n",
              " (53, '('),\n",
              " (1, 'var_1'),\n",
              " (53, ')'),\n",
              " (53, ':'),\n",
              " (4, '\\n'),\n",
              " (5, '    '),\n",
              " (1, 'return'),\n",
              " (1, 'var_1'),\n",
              " (53, '.'),\n",
              " (1, 'split'),\n",
              " (53, '('),\n",
              " (3, \"'\\\\n'\"),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (56, '\\n'),\n",
              " (56, '\\n'),\n",
              " (6, ''),\n",
              " (1, 'def'),\n",
              " (1, 'join'),\n",
              " (53, '('),\n",
              " (1, 'sep'),\n",
              " (53, ','),\n",
              " (1, 'to_join'),\n",
              " (53, ')'),\n",
              " (53, ':'),\n",
              " (4, '\\n'),\n",
              " (5, '    '),\n",
              " (1, 'return'),\n",
              " (1, 'sep'),\n",
              " (53, '.'),\n",
              " (1, 'join'),\n",
              " (53, '('),\n",
              " (1, 'to_join'),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (56, '\\n'),\n",
              " (56, '\\n'),\n",
              " (6, ''),\n",
              " (1, 'def'),\n",
              " (1, 'extract'),\n",
              " (53, '('),\n",
              " (1, 'var_2'),\n",
              " (53, ')'),\n",
              " (53, ':'),\n",
              " (4, '\\n'),\n",
              " (5, '    '),\n",
              " (1, 'var_3'),\n",
              " (53, '='),\n",
              " (1, 'split_lines'),\n",
              " (53, '('),\n",
              " (1, 'var_2'),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (56, '\\n'),\n",
              " (1, 'pieces'),\n",
              " (53, '='),\n",
              " (53, '['),\n",
              " (53, ']'),\n",
              " (4, '\\n'),\n",
              " (56, '\\n'),\n",
              " (1, 'for'),\n",
              " (1, 'line'),\n",
              " (1, 'in'),\n",
              " (1, 'var_3'),\n",
              " (53, ':'),\n",
              " (4, '\\n'),\n",
              " (5, '            '),\n",
              " (1, 'continue'),\n",
              " (4, ''),\n",
              " (6, ''),\n",
              " (6, ''),\n",
              " (0, '')]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sZedgyzu7cD",
        "outputId": "b7f76aa2-4c0f-4120-e359-d4bd7740e0e6"
      },
      "source": [
        "print(vars(train_data.examples[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Input': [' ', 'write', 'a', 'python', 'program', 'to', 'add', 'two', 'numbers'], 'Output': [(57, 'utf-8'), (1, 'num1'), (53, '='), (2, '1.5'), (4, '\\n'), (1, 'var_1'), (53, '='), (2, '6.3'), (4, '\\n'), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'var_1'), (4, '\\n'), (1, 'print'), (53, '('), (3, \"f'Sum: {sum}'\"), (53, ')'), (4, ''), (0, '')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njS_msug6eQ3"
      },
      "source": [
        "# Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-C0ntd8x1e"
      },
      "source": [
        "Our transformer can be understood in terms of its three components:\n",
        "1. An Encoder that encodes an input sequence into state representation vectors.\n",
        "2. An Attention mechanism that enables our Transformer model to focus on the right aspects of the sequential input stream. This is used repeatedly within both the encoder and the decoder to help them contextualize the input data.\n",
        "3. A Decoder that decodes the state representation vector to generate the target output sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSjhBKP59ntL"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgvD_MpkC2OS"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-encoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYN9WDA9Hh_"
      },
      "source": [
        "Our Encoder accepts a batch of source sequences and sequence masks as input. The source mask contains 1 in locations where the input sequence has valid values and 0 where the input sequence has <pad> values. This ensures that the attention mechanism within the encoder does not pay attention to <pad> values.\n",
        "\n",
        "We convert our source sequence tokens into embeddings(tok_embedding) of hid_dim length. Since were are not using any recurrent networks we need to tag each token with its positional indices in order to preserve sequential information. We create an indices tensor(i.e. pos) and convert this into an embedding(pos_embedding) of length hid_dim. This is combined with the source sequence embeddings to create our initial Encoder Layer input tensor src. This src tensor is passed through a series of Encoder Layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE6JimgOCz-w"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1E12pQr9REU"
      },
      "source": [
        "An EncoderLayer is the basic building block of our Transformers Encoder component. Our src tensor along with its src_mask are sent into a multi-head self-attention operation to help our model focus on the necessary aspects of the src tensor. The output from the attention operation is combined with the src tensor(via skip connection) and normalized to avoid vanishing/exploding gradients(during training). This combined output is sent into a PositionwiseFeedForwardLayer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LheiXWVFDEg"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h_Iqnk4Jg5k"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-attention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjEVmTzG9uZ4"
      },
      "source": [
        "A PositionwiseFeedForwardLayer takes the combined input and processes it further using two fully connected layers and a Relu activation function between them. This in combination with the src embedding is the final output of an EncoderLayer. This process repeats for each EncoderLayer block.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9w9xDUKL7LU"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VroGxzNo-GGI"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3MtR2yd-Iaz"
      },
      "source": [
        "Attention is a mechanism that allows a model to focus on the necessary parts of the input sequence as per the demands of the task at hand.\n",
        "\n",
        "Researchers at google like to look at everything as an information retrieval problem. Therefore the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper tries to look at attention in terms of Query, Keys and Values. A search engine accepts a Query and tries to match it up with Indices(i.e. Keys) in order to get appropriate values as results for the query. Similarly one can think of attention as a mechanism in which the query vector and key vector work towards getting the right attention weights(i.e. values).\n",
        "\n",
        "When multiple channels(or heads) of attention are applied in parallel to a single source, it is known as multi-head attention. This increases the learning capacity of the model and therefore leads to better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZmeHfGhGzkN"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBFqGg5z-o_r"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbTr7YPSMRpC"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaHaOGNm_Isc"
      },
      "source": [
        "The architecture of a Decoder is very similar to that of the encoder with the significant differences resulting from the presence of input from two sources, the target sequence and the state representation vector from the encoder. Much like how we had an EncoderLayer block for Encoder, we will be having a DecoderLayer that accepts as input the combination of the embedding from the target token sequence(tok_embedding) and embedding of positional indices for these tokens. And as mentioned earlier, the encoders output also acts as one of the inputs to the DecoderLayer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWBMMF45MMNS"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 10000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcK1-4qZ_WBM"
      },
      "source": [
        "The DecoderLayer forms the building block of our Transformers decoder. Each DecoderLayer involves two attention operations:\n",
        "1. Self-attention on trg embedding.\n",
        "2. Multi-head attention operation that uses the trg as query vector and the encoder outputs act as the key and value vectors.\n",
        "\n",
        "The presence of an extra Multi-head attention operation differentiates the DecoderLayer from an EncoderLayer.\n",
        "\n",
        "The attention outputs from self-attention are normalized and combined with the trg embedding using a residual connection. This is then sent into the multi-head attention operation along with the encoder outputs. The attention layer outputs are then combined with the trg input again and normalized before sending it into the position-wise feedforward layer to generate the final outputs of the DecoderLayer.\n",
        "\n",
        "The purpose of all normalization operations is to prevent vanishing/exploding gradients during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMEr1IFUMxco"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpPhQ2UN8oQ"
      },
      "source": [
        "The main class that implements a transformer for seq2seq problems is given below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr3Mg8OGN6ul"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvrK6zbF_2Fs"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zsZjSSWOSHc"
      },
      "source": [
        "INPUT_DIM = len(Input.vocab)\n",
        "OUTPUT_DIM = len(Output.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 16\n",
        "DEC_HEADS = 16\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vG-tI737e6",
        "outputId": "9146d841-7b03-4748-8662-a809bf23e7ad"
      },
      "source": [
        "len(Output.vocab.__dict__['freqs'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12896"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYVZYDVcOUGK"
      },
      "source": [
        "SRC_PAD_IDX = Input.vocab.stoi[Input.pad_token]\n",
        "TRG_PAD_IDX = Output.vocab.stoi[Output.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd0ePzj0OzLa",
        "outputId": "97e78c48-dada-4ce1-b313-dcc1e5d79e44"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 15,959,396 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZ0hyo8O0vE"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRtAM9Y4O2N2"
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEpApG3YO3ZE"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cvaGRg_-Ml"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs2XdrveCtDD"
      },
      "source": [
        "We have used augmentations in our dataset to mask variable literals. This means that our model can predict a variety of values for a particular variable and all of them are correct as long as the predictions are consistent through the code. This would mean that our training labels are not very certain and hence it would make more sense to treat them to be correct with probability ```1- smooth_eps``` and incorrect otherwise. This is what label smoothening enables us to do. The following is the implementation of CrossEntropyLoss with label smoothening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNfxmsLxD7yb"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossEntropyLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"CrossEntropyLoss - with ability to recieve distrbution as targets, and optional label smoothing\"\"\"\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, reduction='mean', smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "        super(CrossEntropyLoss, self).__init__(weight=weight,\n",
        "                                               ignore_index=ignore_index, reduction=reduction)\n",
        "        self.smooth_eps = smooth_eps\n",
        "        self.smooth_dist = smooth_dist\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def forward(self, input, target, smooth_dist=None):\n",
        "        if smooth_dist is None:\n",
        "            smooth_dist = self.smooth_dist\n",
        "        return cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index,\n",
        "                             reduction=self.reduction, smooth_eps=self.smooth_eps,\n",
        "                             smooth_dist=smooth_dist, from_logits=self.from_logits)\n",
        "\n",
        "\n",
        "def cross_entropy(inputs, target, weight=None, ignore_index=-100, reduction='mean',\n",
        "                  smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "    \"\"\"cross entropy loss, with support for target distributions and label smoothing https://arxiv.org/abs/1512.00567\"\"\"\n",
        "    smooth_eps = smooth_eps or 0\n",
        "\n",
        "    # ordinary log-liklihood - use cross_entropy from nn\n",
        "    if _is_long(target) and smooth_eps == 0:\n",
        "        if from_logits:\n",
        "            return F.cross_entropy(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "        else:\n",
        "            return F.nll_loss(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    if from_logits:\n",
        "        # log-softmax of inputs\n",
        "        lsm = F.log_softmax(inputs, dim=-1)\n",
        "    else:\n",
        "        lsm = inputs\n",
        "\n",
        "    masked_indices = None\n",
        "    num_classes = inputs.size(-1)\n",
        "\n",
        "    if _is_long(target) and ignore_index >= 0:\n",
        "        masked_indices = target.eq(ignore_index)\n",
        "\n",
        "    if smooth_eps > 0 and smooth_dist is not None:\n",
        "        if _is_long(target):\n",
        "            target = onehot(target, num_classes).type_as(inputs)\n",
        "        if smooth_dist.dim() < target.dim():\n",
        "            smooth_dist = smooth_dist.unsqueeze(0)\n",
        "        target.lerp_(smooth_dist, smooth_eps)\n",
        "\n",
        "    if weight is not None:\n",
        "        lsm = lsm * weight.unsqueeze(0)\n",
        "\n",
        "    if _is_long(target):\n",
        "        eps_sum = smooth_eps / num_classes\n",
        "        eps_nll = 1. - eps_sum - smooth_eps\n",
        "        likelihood = lsm.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n",
        "        loss = -(eps_nll * likelihood + eps_sum * lsm.sum(-1))\n",
        "    else:\n",
        "        loss = -(target * lsm).sum(-1)\n",
        "\n",
        "    if masked_indices is not None:\n",
        "        loss.masked_fill_(masked_indices, 0)\n",
        "\n",
        "    if reduction == 'sum':\n",
        "        loss = loss.sum()\n",
        "    elif reduction == 'mean':\n",
        "        if masked_indices is None:\n",
        "            loss = loss.mean()\n",
        "        else:\n",
        "            loss = loss.sum() / float(loss.size(0) - masked_indices.sum())\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def onehot(indexes, N=None, ignore_index=None):\n",
        "    \"\"\"\n",
        "    Creates a one-representation of indexes with N possible entries\n",
        "    if N is not specified, it will suit the maximum index appearing.\n",
        "    indexes is a long-tensor of indexes\n",
        "    ignore_index will be zero in onehot representation\n",
        "    \"\"\"\n",
        "    if N is None:\n",
        "        N = indexes.max() + 1\n",
        "    sz = list(indexes.size())\n",
        "    output = indexes.new().byte().resize_(*sz, N).zero_()\n",
        "    output.scatter_(-1, indexes.unsqueeze(-1), 1)\n",
        "    if ignore_index is not None and ignore_index >= 0:\n",
        "        output.masked_fill_(indexes.eq(ignore_index).unsqueeze(-1), 0)\n",
        "    return output\n",
        "\n",
        "def _is_long(x):\n",
        "    if hasattr(x, 'data'):\n",
        "        x = x.data\n",
        "    return isinstance(x, torch.LongTensor) or isinstance(x, torch.cuda.LongTensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnWtafKlAhb_"
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    # print(inp.shape, target.shape, mask.sum())\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = CrossEntropyLoss(ignore_index = TRG_PAD_IDX, smooth_eps=0.20)\n",
        "    loss = crossEntropy(inp, target)\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Dy_wWrO46l"
      },
      "source": [
        "criterion = maskNLLLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGdcEFmAxlO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itmoGDi_tN74"
      },
      "source": [
        "In order to re-apply our augmentations differently in every epoch we re-create our dataset and dataloaders at the start of each epoch. This regularizes our training process and helps us come up with better models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycBBiEpuO6cG"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def make_trg_mask(trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "        # print(batch)\n",
        "        loss = 0\n",
        "        src = batch.Input.permute(1, 0)\n",
        "        trg = batch.Output.permute(1, 0)\n",
        "        trg_mask = make_trg_mask(trg)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        mask_loss, nTotal = criterion(output, trg, trg_mask)\n",
        "        \n",
        "        mask_loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        print_losses.append(mask_loss.item() * nTotal)\n",
        "        n_totals += nTotal\n",
        "\n",
        "\n",
        "        \n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi3Ev8gaO79_"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "            src = batch.Input.permute(1, 0)\n",
        "            trg = batch.Output.permute(1, 0)\n",
        "            trg_mask = make_trg_mask(trg)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            mask_loss, nTotal = criterion(output, trg, trg_mask)\n",
        "\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "        \n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuB4JqQRO9Wg"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aax76Ie4O_Cr",
        "outputId": "758990b5-b456-4777-fc94-71a85aca7715"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_example = []\n",
        "    val_example = []\n",
        "\n",
        "    for i in range(train_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([train_df.question[i], train_df.solution[i]], fields)\n",
        "            train_example.append(ex)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for i in range(val_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([val_df.question[i], val_df.solution[i]], fields)\n",
        "            val_example.append(ex)\n",
        "        except:\n",
        "            pass       \n",
        "\n",
        "    train_data = data.Dataset(train_example, fields)\n",
        "    valid_data =  data.Dataset(val_example, fields)\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    train_iterator, valid_iterator = BucketIterator.splits((train_data, valid_data), batch_size = BATCH_SIZE, \n",
        "                                                                sort_key = lambda x: len(x.Input),\n",
        "                                                                sort_within_batch=True, device = device)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/EnglishToPython/model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.36it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 2s\n",
            "\tTrain Loss: 4.929 | Train PPL: 138.225\n",
            "\t Val. Loss: 4.391 |  Val. PPL:  80.687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.36it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 1m 58s\n",
            "\tTrain Loss: 4.251 | Train PPL:  70.204\n",
            "\t Val. Loss: 4.189 |  Val. PPL:  65.961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.38it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 1m 58s\n",
            "\tTrain Loss: 4.039 | Train PPL:  56.792\n",
            "\t Val. Loss: 4.070 |  Val. PPL:  58.557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 1m 58s\n",
            "\tTrain Loss: 3.886 | Train PPL:  48.720\n",
            "\t Val. Loss: 3.948 |  Val. PPL:  51.828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.35it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 1m 58s\n",
            "\tTrain Loss: 3.752 | Train PPL:  42.601\n",
            "\t Val. Loss: 3.876 |  Val. PPL:  48.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.38it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 1m 58s\n",
            "\tTrain Loss: 3.643 | Train PPL:  38.192\n",
            "\t Val. Loss: 3.846 |  Val. PPL:  46.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 1m 58s\n",
            "\tTrain Loss: 3.550 | Train PPL:  34.804\n",
            "\t Val. Loss: 3.809 |  Val. PPL:  45.096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 1m 58s\n",
            "\tTrain Loss: 3.471 | Train PPL:  32.164\n",
            "\t Val. Loss: 3.799 |  Val. PPL:  44.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.35it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 1m 59s\n",
            "\tTrain Loss: 3.411 | Train PPL:  30.291\n",
            "\t Val. Loss: 3.781 |  Val. PPL:  43.876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.35it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 1m 58s\n",
            "\tTrain Loss: 3.350 | Train PPL:  28.509\n",
            "\t Val. Loss: 3.817 |  Val. PPL:  45.462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.38it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11 | Time: 1m 58s\n",
            "\tTrain Loss: 3.297 | Train PPL:  27.040\n",
            "\t Val. Loss: 3.763 |  Val. PPL:  43.074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.36it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12 | Time: 1m 58s\n",
            "\tTrain Loss: 3.254 | Train PPL:  25.896\n",
            "\t Val. Loss: 3.744 |  Val. PPL:  42.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.35it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13 | Time: 1m 58s\n",
            "\tTrain Loss: 3.214 | Train PPL:  24.889\n",
            "\t Val. Loss: 3.760 |  Val. PPL:  42.956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.40it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14 | Time: 1m 57s\n",
            "\tTrain Loss: 3.188 | Train PPL:  24.247\n",
            "\t Val. Loss: 3.725 |  Val. PPL:  41.479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15 | Time: 1m 58s\n",
            "\tTrain Loss: 3.148 | Train PPL:  23.292\n",
            "\t Val. Loss: 3.766 |  Val. PPL:  43.187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.39it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16 | Time: 1m 58s\n",
            "\tTrain Loss: 3.128 | Train PPL:  22.837\n",
            "\t Val. Loss: 3.751 |  Val. PPL:  42.542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.36it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17 | Time: 1m 58s\n",
            "\tTrain Loss: 3.104 | Train PPL:  22.280\n",
            "\t Val. Loss: 3.742 |  Val. PPL:  42.187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.39it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18 | Time: 1m 58s\n",
            "\tTrain Loss: 3.081 | Train PPL:  21.772\n",
            "\t Val. Loss: 3.748 |  Val. PPL:  42.433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.38it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19 | Time: 1m 58s\n",
            "\tTrain Loss: 3.064 | Train PPL:  21.421\n",
            "\t Val. Loss: 3.742 |  Val. PPL:  42.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.40it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20 | Time: 1m 57s\n",
            "\tTrain Loss: 3.039 | Train PPL:  20.881\n",
            "\t Val. Loss: 3.749 |  Val. PPL:  42.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21 | Time: 1m 58s\n",
            "\tTrain Loss: 3.025 | Train PPL:  20.601\n",
            "\t Val. Loss: 3.725 |  Val. PPL:  41.491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:49<00:00,  4.46it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22 | Time: 1m 55s\n",
            "\tTrain Loss: 3.009 | Train PPL:  20.266\n",
            "\t Val. Loss: 3.717 |  Val. PPL:  41.153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.45it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23 | Time: 1m 56s\n",
            "\tTrain Loss: 2.995 | Train PPL:  19.994\n",
            "\t Val. Loss: 3.732 |  Val. PPL:  41.758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.42it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24 | Time: 1m 56s\n",
            "\tTrain Loss: 2.984 | Train PPL:  19.761\n",
            "\t Val. Loss: 3.699 |  Val. PPL:  40.411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.40it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25 | Time: 1m 57s\n",
            "\tTrain Loss: 2.973 | Train PPL:  19.549\n",
            "\t Val. Loss: 3.731 |  Val. PPL:  41.733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.42it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26 | Time: 1m 57s\n",
            "\tTrain Loss: 2.960 | Train PPL:  19.305\n",
            "\t Val. Loss: 3.748 |  Val. PPL:  42.435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.40it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27 | Time: 1m 57s\n",
            "\tTrain Loss: 2.950 | Train PPL:  19.104\n",
            "\t Val. Loss: 3.736 |  Val. PPL:  41.910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.42it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 28 | Time: 1m 56s\n",
            "\tTrain Loss: 2.945 | Train PPL:  19.001\n",
            "\t Val. Loss: 3.757 |  Val. PPL:  42.838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.43it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29 | Time: 1m 56s\n",
            "\tTrain Loss: 2.933 | Train PPL:  18.781\n",
            "\t Val. Loss: 3.734 |  Val. PPL:  41.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.39it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30 | Time: 1m 57s\n",
            "\tTrain Loss: 2.921 | Train PPL:  18.561\n",
            "\t Val. Loss: 3.743 |  Val. PPL:  42.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.43it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31 | Time: 1m 57s\n",
            "\tTrain Loss: 2.914 | Train PPL:  18.422\n",
            "\t Val. Loss: 3.722 |  Val. PPL:  41.346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.44it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 32 | Time: 1m 56s\n",
            "\tTrain Loss: 2.909 | Train PPL:  18.329\n",
            "\t Val. Loss: 3.724 |  Val. PPL:  41.430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.39it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33 | Time: 1m 58s\n",
            "\tTrain Loss: 2.901 | Train PPL:  18.185\n",
            "\t Val. Loss: 3.736 |  Val. PPL:  41.910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.44it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 34 | Time: 1m 56s\n",
            "\tTrain Loss: 2.892 | Train PPL:  18.034\n",
            "\t Val. Loss: 3.746 |  Val. PPL:  42.363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.43it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35 | Time: 1m 56s\n",
            "\tTrain Loss: 2.887 | Train PPL:  17.942\n",
            "\t Val. Loss: 3.741 |  Val. PPL:  42.153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:49<00:00,  4.47it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36 | Time: 1m 55s\n",
            "\tTrain Loss: 2.874 | Train PPL:  17.715\n",
            "\t Val. Loss: 3.739 |  Val. PPL:  42.070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.42it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 37 | Time: 1m 56s\n",
            "\tTrain Loss: 2.873 | Train PPL:  17.698\n",
            "\t Val. Loss: 3.755 |  Val. PPL:  42.713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.43it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 38 | Time: 1m 57s\n",
            "\tTrain Loss: 2.860 | Train PPL:  17.461\n",
            "\t Val. Loss: 3.741 |  Val. PPL:  42.156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.41it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 39 | Time: 1m 57s\n",
            "\tTrain Loss: 2.860 | Train PPL:  17.457\n",
            "\t Val. Loss: 3.716 |  Val. PPL:  41.115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:52<00:00,  4.37it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40 | Time: 1m 58s\n",
            "\tTrain Loss: 2.849 | Train PPL:  17.265\n",
            "\t Val. Loss: 3.705 |  Val. PPL:  40.661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.45it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41 | Time: 1m 56s\n",
            "\tTrain Loss: 2.850 | Train PPL:  17.294\n",
            "\t Val. Loss: 3.741 |  Val. PPL:  42.128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.44it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 42 | Time: 1m 56s\n",
            "\tTrain Loss: 2.844 | Train PPL:  17.180\n",
            "\t Val. Loss: 3.741 |  Val. PPL:  42.156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.41it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 43 | Time: 1m 57s\n",
            "\tTrain Loss: 2.836 | Train PPL:  17.050\n",
            "\t Val. Loss: 3.717 |  Val. PPL:  41.151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.44it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 44 | Time: 1m 56s\n",
            "\tTrain Loss: 2.834 | Train PPL:  17.010\n",
            "\t Val. Loss: 3.707 |  Val. PPL:  40.722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.42it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 45 | Time: 1m 57s\n",
            "\tTrain Loss: 2.827 | Train PPL:  16.903\n",
            "\t Val. Loss: 3.758 |  Val. PPL:  42.868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:50<00:00,  4.43it/s]\n",
            "100%|| 19/19 [00:01<00:00, 15.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 46 | Time: 1m 56s\n",
            "\tTrain Loss: 2.824 | Train PPL:  16.851\n",
            "\t Val. Loss: 3.759 |  Val. PPL:  42.901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:51<00:00,  4.39it/s]\n",
            "100%|| 19/19 [00:01<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 47 | Time: 1m 57s\n",
            "\tTrain Loss: 2.820 | Train PPL:  16.769\n",
            "\t Val. Loss: 3.742 |  Val. PPL:  42.200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:53<00:00,  4.30it/s]\n",
            "100%|| 19/19 [00:01<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 48 | Time: 2m 0s\n",
            "\tTrain Loss: 2.812 | Train PPL:  16.650\n",
            "\t Val. Loss: 3.734 |  Val. PPL:  41.848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:55<00:00,  4.23it/s]\n",
            "100%|| 19/19 [00:01<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 49 | Time: 2m 2s\n",
            "\tTrain Loss: 2.812 | Train PPL:  16.645\n",
            "\t Val. Loss: 3.753 |  Val. PPL:  42.644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 490/490 [01:55<00:00,  4.23it/s]\n",
            "100%|| 19/19 [00:01<00:00, 14.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50 | Time: 2m 2s\n",
            "\tTrain Loss: 2.809 | Train PPL:  16.596\n",
            "\t Val. Loss: 3.754 |  Val. PPL:  42.680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMKkB13yPeJ7"
      },
      "source": [
        "SRC = Input\n",
        "TRG = Output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieIjql9uPKH1"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50000):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mlHzigPJpLT"
      },
      "source": [
        "## Displaying Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTBipndfPOlX"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(30,50))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5dlnw-2aTcs",
        "outputId": "d0ef2f04-2c13-4510-f1d0-95d279795d62"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/EnglishToPython/model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "sIyQeFx-dMlK",
        "outputId": "58ecdc82-16db-464b-d77c-d58d437419d8"
      },
      "source": [
        "src = \"write a function that adds two numbers\"\n",
        "src=src.split(\" \")\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg sequence: ')\n",
        "print(translation)\n",
        "print(\"code: \\n\", untokenize(translation[:-1]).decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted trg sequence: \n",
            "[(57, 'utf-8'), (1, 'def'), (1, 'add'), (53, '('), (1, 'a'), (53, ','), (1, 'b'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'return'), (1, 'a'), (53, '+'), (1, 'b'), (4, '\\n'), (6, ''), (1, 'def'), (1, 'subtract'), (53, '('), (1, 'a'), (53, ','), (1, 'b'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'return'), (1, 'a'), (53, '-'), (1, 'b'), (4, '\\n'), (6, ''), (1, 'def'), (1, 'multiply'), (53, '('), (1, 'a'), (53, ','), (1, 'b'), (53, ')'), (53, ':'), (4, '\\n'), (5, '    '), (1, 'return'), (1, 'a'), (53, '-'), (1, 'b'), (4, '\\n'), (6, ''), (1, 'def'), (1, 'multiply'), (53, '('), (1, 'a'), (53, ','), (1, 'b'), (53, ')'), (53, ':'), (4, '\\n'), (5, '        '), (1, 'return'), (1, 'a'), (4, '\\n'), (6, ''), (1, 'return'), (1, 'a'), (4, '\\n'), (56, '\\n'), (6, ''), (1, 'a'), (53, '='), (53, '['), (2, '2'), (53, ','), (2, '4'), (53, ','), (2, '6'), (53, ','), (2, '8'), (53, ']'), (4, ''), (0, ''), '<eos>']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-e6693720d7eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'predicted trg sequence: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36muntokenize\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \"\"\"\n\u001b[1;32m    332\u001b[0m     \u001b[0mut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUntokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36muntokenize\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mtok_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tokenize.py\u001b[0m in \u001b[0;36mcompat\u001b[0;34m(self, token, iterable)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtoknum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEDENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mindents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtoknum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNEWLINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHWqhmvtPTJv"
      },
      "source": [
        "display_attention(src, translation, attention)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhIC2mgfv37B"
      },
      "source": [
        "# Sample Outputs for English to Python translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lwaEkWxK8DA"
      },
      "source": [
        "Lets load our pretrained model to perform inference on a set of examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_d2MUwHK62D",
        "outputId": "e072cae8-dd80-47d1-c3ef-db183726ca85"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/EnglishToPython/model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vveLENmdLEE4"
      },
      "source": [
        "Function that translates an English src string to python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiYFFxnzj6Mn"
      },
      "source": [
        "def eng_to_python(src):\n",
        "  src=src.split(\" \")\n",
        "  translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "  print(f'predicted trg: \\n')\n",
        "  # print(translation)\n",
        "  print(untokenize(translation[:-1]).decode('utf-8'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYIOx1D-KIZ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}